{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9545c16-a16f-41ed-a346-ced21aedd587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\anaconda\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\anaconda\\lib\\site-packages (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\anaconda\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\anaconda\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\anaconda\\lib\\site-packages (from requests) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\anaconda\\lib\\site-packages (from requests) (2024.2.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\anaconda\\lib\\site-packages (from beautifulsoup4) (2.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbd5732a-7e18-4f19-86e7-627eff63f76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "크롤링 날짜 리스트: ['2025-05-04', '2025-05-11', '2025-05-18', '2025-05-25']\n",
      "총 4개의 일자를 크롤링합니다...\n",
      "\n",
      "2025-05-04 → 크롤링 시작...\n",
      "2025-05-04: 100곡 수집 성공\n",
      "2025-05-11 → 크롤링 시작...\n",
      "2025-05-11: 100곡 수집 성공\n",
      "2025-05-18 → 크롤링 시작...\n",
      "2025-05-18: 100곡 수집 성공\n",
      "2025-05-25 → 크롤링 시작...\n",
      "2025-05-25: 100곡 수집 성공\n",
      "\n",
      " 날짜별 크롤링 요약:\n",
      " - 2025-05-04 | 성공 | 100곡\n",
      " - 2025-05-11 | 성공 | 100곡\n",
      " - 2025-05-18 | 성공 | 100곡\n",
      " - 2025-05-25 | 성공 | 100곡\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "def get_date_list(start_date_str, end_date_str):\n",
    "    start_date = datetime.strptime(start_date_str, \"%Y-%m-%d\")\n",
    "    end_date = datetime.strptime(end_date_str, \"%Y-%m-%d\")\n",
    "    if start_date.weekday() != 6:\n",
    "        raise ValueError(\"start_date must be a Sunday (weekday=6)\")\n",
    "    if end_date.weekday() != 5:\n",
    "        raise ValueError(\"end_date must be a Saturday (weekday=5)\")\n",
    "    date_list = []\n",
    "\n",
    "    while start_date <= end_date:\n",
    "        date_list.append(start_date.strftime(\"%Y-%m-%d\"))\n",
    "        start_date += timedelta(days=7)  # 주간차트 얻기 위함.\n",
    "\n",
    "    return date_list\n",
    "\n",
    "def scrape_billboard(date):\n",
    "    url = f\"https://www.billboard.com/charts/hot-100/{date}\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    chart_items = soup.select(\"div.o-chart-results-list-row-container\")\n",
    "\n",
    "    weekly_results = []\n",
    "\n",
    "    for item in chart_items[:100]:  # 최대 100위까지 시도\n",
    "        try:\n",
    "            ranking = int(item.select_one(\"span.c-label.a-font-primary-bold-l\").text.strip())\n",
    "            title = item.select_one(\"h3.c-title\").text.strip()\n",
    "            singer = item.select_one(\"span.c-label.a-no-trucate.a-font-primary-s\").text.strip()\n",
    "\n",
    "            weekly_results.append({\n",
    "                \"basedt\": date,\n",
    "                \"music_rank\": ranking,\n",
    "                \"music_name\": title,\n",
    "                \"artist_name\": singer\n",
    "            })\n",
    "\n",
    "        except (AttributeError,ValueError):\n",
    "            continue  # 일부 항목 누락 시 무시\n",
    "\n",
    "    return weekly_results\n",
    "\n",
    "def crawl_billboard_range(start_date, end_date, delay_sec=3):\n",
    "    all_results = []\n",
    "    status_report = []\n",
    "    date_list = get_date_list(start_date, end_date)\n",
    "\n",
    "    print(f\"크롤링 날짜 리스트: {date_list}\")\n",
    "    print(f\"총 {len(date_list)}개의 일자를 크롤링합니다...\\n\")\n",
    "\n",
    "    for date in date_list:\n",
    "        print(f\"{date} → 크롤링 시작...\")\n",
    "        try:\n",
    "            results = scrape_billboard(date)\n",
    "            count = len(results)\n",
    "\n",
    "            if count > 0:\n",
    "                print(f\"{date}: {count}곡 수집 성공\")\n",
    "                status_report.append((date, \"성공\", count))\n",
    "            else:\n",
    "                print(f\"{date}: 곡 수집 실패 또는 차트 없음\")\n",
    "                status_report.append((date, \"실패\", 0))\n",
    "\n",
    "            all_results.extend(results)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"{date}: 에러 발생 → {e}\")\n",
    "            status_report.append((date, \"에러\", 0))\n",
    "\n",
    "        time.sleep(delay_sec)\n",
    "\n",
    "    print(\"\\n 날짜별 크롤링 요약:\")\n",
    "    for date, status, count in status_report:\n",
    "        print(f\" - {date} | {status} | {count}곡\")\n",
    "\n",
    "    return all_results\n",
    "\n",
    "\n",
    "# 사용 예시\n",
    "start_date = \"2025-05-04\"\n",
    "end_date = \"2025-05-31\"\n",
    "\n",
    "\n",
    "data = crawl_billboard_range(start_date, end_date)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
